# 빅 오 분석법

입력 값의 개수에 따라 알고리즘이 수행되는데 걸리는 시간을 바탕으로 알고리즘의 효율성을 평가하는 실행시간 분석법이다.

주로 면접에서 문제에 대한 답을 내놓고 나서, 구현의 효율성에 대한 이야기를 나눌 때 **어떤 상황에서 어떤 구현방법**이 더 유리할지를 이야기할 때 빅오표기법을 이용하여 설명하도록 한다. 특히나 메모리나 공간사용, 그리고 재귀 함수 호출 관련해서 헷갈리는 부분이 있다면 꼭 짚고 넘어가자.

## 빅오 분석

두 함수가 있을 때, 구현해보고 벤치마킹 하는 것은 비효율적이기도 하면서, 실용적인 면에서도 그리 좋은 방법이 아니다. 그래서 직접 구현하지 않아도 알고리즘의 성능을 예측할 수 있어야한다.

특히나 알고리즘에서는 입력값의 개수 == 입력크기(n)가 작으면 웬만하면 모두 빠르기 때문에, lim n->infinite(y=x^k)처럼 입력크기가 커질 수록 두드러진다.

빅 오 분석법은 크기가 커질수록 두드러지는 경우이므로, 작은 크기의 입력값을 여러번 처리해야하는 경우에는 빅 오 뿐만 아니라 다른 알고리즘의 효율도 따져야한다.

## 빅오 분석법의 원리

1. **n : 입력값의 크기(개수) 정형화**  
   : 문제에 따라 n은 연결리스트의 노드 개수, 특정 자료형의 비트 수, 해시 테이블의 항목개수, 입력값의 크기 등... 이 될 수 있다.
2. **n개의 입력된 값에 대해 연산횟수를 n의 식으로 표현**  
   위에서 설명했듯, 빅 오 분석법은 크기가 커질 수록 두드러진다. 그에 따라 O(n+2)는 O(n)이라고 봐도 무관하다. 상수는 보통 값의 조건판단, 분기, 초기화가 반복문이 아닌 line만큼 이루어지는 경우 기재한다.  
    이를 일반화 하여, *최고차항 n이 매우 커질 떄 가장 큰 항만 남기고 다른 항은 다 무시*한다.

실제로 O(n)과 O(n^2)알고리즘을 벤치마킹해보자.

비교해야하는 원소의 개수가 30,000개라면 O(n)은 30,000번 비교하면 되지만 O(n^2)는 900,000,000번 비교연산을 필요로한다. 따라서 비교횟수가 30,000분의 1정도로 적기 떄문에 O(n)이 훨씬 빠르다.

벤치마크결과 O(n)은 0.01초도 안걸려서 실행이 끝나지만, O(n^2)는 23.99초가 걸리는 것을 확인할 수 있다.

## 최선, 평균, 최악 케이스

그렇다면 O(n^2)의 경우에, 최댓값이 중간에 있는 경우에는 n\*n/2만큼만 비교하면 되므로 좀 더 효율이 좋은 경우가 아닌가?

물론 빅 오 표기법에서는 상수가 분모에 있던, 분자에 있던, 상수로 남던 무시하여 n차원 단항식으로만 표현한다. 하지만 실제로 최선/평균/최악의 케이스에 따라 효율적인 알고리즘이 다를 수 있다. 그래서 이러한 케이스를 따져주어야한다.

예를 들어, 최댓값을 찾는 알고리즘을 O(n), O(n^2) 총 두가지로 작성했다. O(n^2)의 경우에는 for문의 중첩으로 이루어진 경우이기 때문에 가장 좋은 케이스(ex. 최댓값 판단하는 코드가 맨 앞에 존재하고, 바로 최댓값이 맨 첫번째 입력값으로 주어진다거나.. )의 경우에는 O(n^(2-1))이 될 수 있지만, 최악에는 O(n^2)이다. 차수가 하나 줄어들었다고 해서 한바퀴 순회를 무조건 해야한다는 점에서는 O(n)배열보다 유리하다고 볼 수는 없다. 어차피 모든 수를 순회해야한다. 는 점에서는 틀림없기 때문이다.

오히려 차수가 증가할 수록 최선, 평균, 최악의 케이스에 대한 생각을 하고 코드를 짜야한다는 것이다.

🌱 특히나 정렬 알고리즘의 경우, **어떤 시나리오에 가장 중점을 둬야 할지**를 아는 것도 중요한 방법이다. 정렬 알고리즘 중에서도 정렬이 되지 않은 데이터에 대한 최악의 케이스에는 성능이 정말 나쁘지만, 이미 정렬된 데이터가 들어왔을때는 성능이 최선인 경우도 있다. 또 다른 경우로 정렬된 데이터가 들어와도 항상 모든 순회를 하는 경우도 존재한다.

## 최적화와 빅 오 분석법

알고리즘을 최적화한다고 해서 항상 전체 실행 시간이 빨라지지는 않는다. 여기서의 최적화란, 결국 생각하기로 최댓값을 찾을 때 모든 숫자를 순회하지 않고 뒤의 값과의 비교만 진행해보자! 라고 했을 때 어차피 모든 숫자를 순회해야한다는 점에서는 다른 점이 없다는 것이다.
위에서 설명한 것들과 같은 맥락이다.

## 빅 오 분석법을 적용하는 방법

1. 입력값이 무엇인지 확인하고 어떤 것을 n으로 놓을지 결정한다
2. 알고리즘에서 수행해야할 연산 횟수를 n의 식으로 표현한다
3. 차수가 제일 높은 항만 남긴다
4. 모든 상수 인수를 없앤다

입력 데이터의 크기에 의존하는 연산을 파악하기만 한다면, 빅 오 분석법은 스무스하게 넘어갈것이다. 사실상 현재의 빅 오 정의는 Big-Theta의 정의에 좀 더 가깝고, 이렇게 자주 사용하기도 하여 빅세타를 위주로 공부했다.

## 그렇다면 어떤 알고리즘이 나을까?

| O(f(n))  | 정의                                                                                                     |
| -------- | -------------------------------------------------------------------------------------------------------- |
| O(log n) | 로그 알고리즘 : 실행시간이 입력크기의 로그에 비례해서 증가                                               |
| O(n)     | 선형(linear) 알고리즘 : 실행시간이 입력크기에 선형 비례하여 증가                                         |
| O(nlogn) | 준선형(quasilinear) 알고리즘 : 속도가 선형알고리즘과 다항식 알고리즘의 중간쯤                            |
| O(n^k)   | 다항식(polynomial) 알고리즘 : 상수k를 제곱수로 가져 증가                                                 |
| O(k^n)   | 지수(exponential) 알고리즘 : 다항식 알고리즘보다도 실행시간이 빠르게 증가                                |
| O(n!)    | 팩토리얼(factorial) 알고리즘 : 가장 느린 알고리즘으로, n이 작아도 금방 거의 쓰기 힘든 수준으로 느려진다. |

보통 준선형알고리즘 `O(nlogn)`이거나 그보다 더 빠른 알고리즘을 찾으면 애플리케이션 성능을 크게 향상시킬 수 있다.

## 메모리 용량 분석

성능을 따지는데는 **실행시간분석 외에도 메모리 용량 분석 또한 중요하다.** 이를 애플리케이션의 메모리 용량(memory footprint)라고 부르기도 하는데, 이는 특히나 실행시간보다 메모리 용량이 더 중요하다고 판단되는 경우에 분석이 필수이다.

빅 오 분석법과 비슷하게 메모리 사용량을 입력크기 n의 식으로 표현하는 접근법을 사용하여 저장공간을 따져보면된다.

혹은 구현방법의 메모리 사용량(memory usage)를 물어볼 수도 있는데, 얼마나 잘 추정해나갈수있을지를 생각해보는게 관건이다. 특히나 VM에서 돌아가는 C#이나 자바같은 언어에서 중요하다. 실제로 자료구조가 어떻게 구현되는지 제대로 이해하는것이 중요하다. C++과 관련되어서라면, 구조체나 클래스에 필요한 메모리에 대한 질문을 받을 수도 있다. 메모리 정렬이나 구조체 패킹등을 이해하고있는지를 확인하기 위한 질문이다.

## ☁️ 그래서 문제를 풀 때는

내가 왜 이러한 자료구조를 썼는지, 이 자료구조를 사용하여 어떠한 장단점이 있는지, 다른 방법으로는 이런것들을 구상해보았지만 구현에는 왜 이 방법을 썼는지. 등을 면접관과 의사소통을 활발히 하면서 알릴 수 있는 것이 좋다.

가장 먼저 할 것은 문제를 확실히 하는것이며 -> 문제에 대한 이해가 됐다면 문제의 입력 / 주어진 상황을 따져보고 -> 몇가지 예를 들어보고 (test case) -> 그 상황이 제대로 돌아간다면, 한번 제출해본다.

또한 문제가 풀다가 중간에 막히면 또다른 예를 시도해보거나, 다른 알고리즘을 적용한다. 또한 언어 자체의 특수한 기능이나 고급 기능을 떠올려보는것도 좋다.
